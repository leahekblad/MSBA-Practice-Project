---
title: "Home Credit Modeling"
author: "Leah Ekblad"
date: "`r Sys.Date()`"
output:
  html_document:
    number_sections: no
    toc: yes
    toc-depth: 3
    toc-title: "Contents"
editor_options: 
  chunk_output_type: console
execute:
  warning: false
  message: false
---

```{r setup, include=FALSE}
# Set CRAN mirror
options(repos = c(CRAN = "https://cloud.r-project.org/"))

install.packages("class")
```

# Data Prep
```{r, message=FALSE, warning = FALSE}
# Load libraries
library(tidyverse)
library(dplyr)
library(DataExplorer) 
library(ggplot2)
library(caret)
library(lattice)
library(skimr)
library(tidymodels)
library(patchwork)
library(missForest)
library(kableExtra)
library(matrixStats)
library(ROSE)
library(car)
library(ggcorrplot)
install.packages("class")
library(class)
library(RWeka)
install.packages("randomForest")
library(randomForest)
install.packages("ranger")
library(ranger)
library(e1071)

# Data
balanced_data <- read.csv("FivePercBalancedTrain.csv") # 2482
imputed_test <- read.csv("ImputedTest.csv") # 48744
imputed_train <- read.csv("ImputedTrain.csv") # 307511
train_target <- imputed_train$TARGET # 307511
train_data <- imputed_train %>% select(-TARGET)
```

# Feature Engineering
```{r, message=FALSE, warning = FALSE}
balanced_data['annuity_income_percentage'] = balanced_data['AMT_ANNUITY'] / balanced_data['AMT_INCOME_TOTAL']
balanced_data['credit_to_annuity_ratio'] = balanced_data['AMT_CREDIT'] / balanced_data['AMT_ANNUITY']
balanced_data['credit_to_goods_ratio'] = balanced_data['AMT_CREDIT'] / balanced_data['AMT_GOODS_PRICE']
balanced_data['credit_to_income_ratio'] = balanced_data['AMT_CREDIT'] / balanced_data['AMT_INCOME_TOTAL']
balanced_data['income_credit_percentage'] = balanced_data['AMT_INCOME_TOTAL'] / balanced_data['AMT_CREDIT']
balanced_data['income_per_person'] = balanced_data['AMT_INCOME_TOTAL'] / balanced_data['CNT_FAM_MEMBERS']
balanced_data['payment_rate'] = balanced_data['AMT_ANNUITY'] / balanced_data['AMT_CREDIT']
balanced_data['phone_to_birth_ratio'] = balanced_data['DAYS_LAST_PHONE_CHANGE'] / balanced_data['DAYS_BIRTH']
```

# Modeling

## Logistic Model 1
```{r, message=FALSE, warning = FALSE}
# Set up logistic regression 
NAME_HOUSING_TYPE.glm <- glm(TARGET ~ NAME_HOUSING_TYPE , data = balanced_data, family = "binomial")
summary(NAME_HOUSING_TYPE.glm)

# Make predictions
predictions <- data.frame(.pred = predict(NAME_HOUSING_TYPE.glm, newdata=imputed_test, type = "response"))

# Prepare data for Kaggle submission
kaggle_submission <- predictions %>%  
  bind_cols(., imputed_test) %>% 
  dplyr::select(SK_ID_CURR, .pred) %>% 
  rename(TARGET=.pred)

# Write Kaggle File
vroom::vroom_write(x=kaggle_submission, file="./NameHousingTypeLog.csv", delim=",")
```
> Kaggle Score: .51602

## Logistic Model 2
```{r, message=FALSE, warning = FALSE}
# Set up logistic regression 
new_logistic_model <- glm(TARGET ~ NAME_CONTRACT_TYPE + 
                          CODE_GENDER +
                          AMT_CREDIT + 
                          AMT_ANNUITY +
                          AMT_GOODS_PRICE + 
                          NAME_FAMILY_STATUS + 
                          REGION_POPULATION_RELATIVE + 
                          DAYS_BIRTH,
                          data = balanced_data, 
                          family = "binomial")
summary(new_logistic_model)

# Make predictions
logistic_predictions <- data.frame(.pred = predict(new_logistic_model, newdata = imputed_test, type = "response"))

# Prepare data for Kaggle submission
logistic_kaggle_submission <- logistic_predictions %>% 
  bind_cols(., imputed_test) %>% 
  select(SK_ID_CURR, .pred) %>% 
  rename(TARGET = .pred)

# Replace NA values with 0 
logistic_kaggle_submission[is.na(logistic_kaggle_submission)] <- 0

# Write Kaggle file
vroom::vroom_write(x=logistic_kaggle_submission, file="./NewLogisticPreds.csv", delim=",")
```
> Kaggle Score: .63927

## Logistic Model 3
```{r, message=FALSE, warning = FALSE}
# Set up logistic regression
AMT_INCOME_TOTAL.glm <- glm(TARGET ~ AMT_INCOME_TOTAL, data = balanced_data, family = "binomial")
summary(AMT_INCOME_TOTAL.glm)

# Make predictions
predictions <- data.frame(.pred = predict(AMT_INCOME_TOTAL.glm, newdata = imputed_test, type = "response"))

# Prepare data for Kaggle submission
kaggle_submission <- predictions %>%  
  bind_cols(., imputed_test) %>%  
  dplyr::select(SK_ID_CURR, .pred) %>%  
  rename(TARGET = .pred)

# Write out Kaggle file
vroom::vroom_write(x=kaggle_submission, file="./amt_income_total.csv", delim=",")
```
> Kaggle Score: .51443

# Naive Bayes
```{r, message=FALSE, warning = FALSE}
# Prepare the target and features for training
train_target <- imputed_train$TARGET  # Extract the target variable
train_data <- imputed_train %>% select(-TARGET)  # Select all features without TARGET

# Set up Naive Bayes model
naive_bayes_model <- naiveBayes(as.factor(train_target) ~ ., data = train_data)

# Make predictions on the test data
naive_bayes_predictions <- predict(naive_bayes_model, newdata = imputed_test)

# Prepare data for Kaggle submission
kaggle_submission <- data.frame(SK_ID_CURR = imputed_test$SK_ID_CURR, TARGET = as.numeric(naive_bayes_predictions) - 1)  # Convert factor to numeric

# Write out the file for Kaggle submission
vroom::vroom_write(x = kaggle_submission, file = "./naive_bayes_predictions.csv", delim = ",")
```
> Kaggle Score: .58287

# Lasso
```{r, message=FALSE, warning = FALSE}
# Load necessary libraries
library(glmnet)  # For Lasso regression

# Load the imputed training and test data
imputed_train <- read.csv("ImputedTrain.csv")  # Should have 307,511 rows
imputed_test <- read.csv("ImputedTest.csv")    # Should have 48,744 rows

# Prepare the target and features for training
train_target <- imputed_train$TARGET  # Extract the target variable
train_data <- imputed_train %>% select(-TARGET)  # Select all features without TARGET

# Convert training data to matrix format for glmnet
train_matrix <- as.matrix(train_data)

# Set up a Lasso model
set.seed(123)  # For reproducibility
lasso_model <- cv.glmnet(train_matrix, train_target, alpha = 1, family = "binomial")  # Use binomial family for binary outcome

# Plot cross-validated mean squared error against log(lambda)
plot(lasso_model)

# Make predictions on the test data
test_matrix <- as.matrix(imputed_test)  # Convert test data to matrix format
lasso_predictions <- predict(lasso_model, newx = test_matrix, s = "lambda.min", type = "response")

# Prepare data for Kaggle submission
kaggle_submission <- data.frame(SK_ID_CURR = imputed_test$SK_ID_CURR, 
                                 TARGET = as.numeric(lasso_predictions > 0.5))  # Convert probabilities to binary

# Write out the file for Kaggle submission
vroom::vroom_write(x = kaggle_submission, file = "./lasso_predictions.csv", delim = ",")
```
> Kaggle Score: .50138

# Random Forrest
```{r, message=FALSE, warning = FALSE}
# Prepare the target and features for training
train_target <- imputed_train$TARGET  # Extract the target variable
train_data <- imputed_train %>% select(-TARGET)  # Select all features without TARGET

# Fit the Random Forest model using all features
set.seed(123)  # For reproducibility
rf_model_ranger <- ranger(train_target ~ ., 
                          data = train_data, 
                          importance = "impurity", 
                          num.trees = 500)

# Print the model summary
print(rf_model_ranger)

# Extract the importance values with the feature names
feature_importance <- rf_model_ranger$variable.importance

# Get the feature names
feature_names <- colnames(train_data)

# Predict on the test data
predictions <- predict(rf_model_ranger, data = imputed_test)$predictions

# Prepare data for Kaggle submission
kaggle_submission <- data.frame(SK_ID_CURR = imputed_test$SK_ID_CURR, TARGET = predictions)

# Write out the file for Kaggle submission
vroom::vroom_write(x = kaggle_submission, file = "./RF.csv", delim = ",")
```
> Kaggle Score: .67362

# Random Forrest: Top 10 Features
```{r, message=FALSE, warning = FALSE}
# Combine feature names with their importance values
feature_importance_df <- data.frame(Feature = feature_names, Importance = feature_importance)

# Sort the dataframe by the Importance column in descending order
feature_importance_df <- feature_importance_df[order(-feature_importance_df$Importance), ]

# View the sorted dataframe
print(feature_importance_df)

# Select the top 10 most important features
top_10_features <- c("EXT_SOURCE_2", "DAYS_BIRTH", "DAYS_ID_PUBLISH","DAYS_REGISTRATION", 
                     "SK_ID_CURR", "AMT_ANNUITY", "DAYS_LAST_PHONE_CHANGE", 
                     "AMT_CREDIT", "EXT_SOURCE_1", "AMT_INCOME_TOTAL")

existing_features <- intersect(top_10_features, colnames(train_data))
train_data_top_10 <- train_data[, existing_features]

# Subset the model_data to include only the top 10 features
train_data_top_10 <- train_data[, top_10_features]

# Fit the Random Forest model again using only the top 10 features
set.seed(123)  # For reproducibility
rf_model_ranger_top_10 <- ranger(train_target ~ ., 
                                 data = train_data_top_10, 
                                 importance = "impurity", 
                                 num.trees = 500)

# Print the new model summary
print(rf_model_ranger_top_10)

# Subset the test data to include only the top 10 features
test_data_top_10 <- imputed_test[, top_10_features]

# Predict on the test data using the top 10 features model
predictions_top_10 <- predict(rf_model_ranger_top_10, data = test_data_top_10)$predictions

# Prepare data for Kaggle submission
kaggle_submission <- data.frame(SK_ID_CURR = imputed_test$SK_ID_CURR, TARGET = predictions_top_10)

# Write out the file for Kaggle submission
vroom::vroom_write(x = kaggle_submission, file = "./RF_top_10_features.csv", delim = ",")
```
> Kaggle Score: .66110

# Random Forrest: 100 Trees
```{r, message=FALSE, warning = FALSE}
# Load necessary libraries
library(ranger)  # For Random Forest

# Prepare the target and features for training
train_target <- imputed_train$TARGET  # Extract the target variable
train_data <- imputed_train %>% select(-TARGET)  # Select all features without TARGET

# Fit a Random Forest model with default parameters and fewer trees
set.seed(123)  # For reproducibility
rf_model_simple <- ranger(
  formula = train_target ~ ., 
  data = train_data, 
  importance = "impurity", 
  num.trees = 100,  # Reduced number of trees for faster training
  min.node.size = 5  # Optional: Adjust to a reasonable size for faster splits
)

# Print the model summary
print(rf_model_simple)

# Make predictions on the test data
predictions <- predict(rf_model_simple, data = imputed_test)$predictions

# Prepare data for Kaggle submission
kaggle_submission <- data.frame(SK_ID_CURR = imputed_test$SK_ID_CURR, TARGET = predictions)

# Write out the file for Kaggle submission
vroom::vroom_write(x = kaggle_submission, file = "./RF_simple_no_tuning.csv", delim = ",")
```
> Kaggle Score: .66366

# Random Forrest: mtry Adjustment
- mtry helps control the trade-offs between bias and variance, enhances model generalization, and improves computational efficiency.
```{r, message=FALSE, warning = FALSE}
# Load necessary libraries
library(ranger)  # For Random Forest

# Prepare the target and features for training
train_target <- imputed_train$TARGET  # Extract the target variable
train_data <- imputed_train %>% select(-TARGET)  # Select all features without TARGET

# Set seed for reproducibility
set.seed(123)

# Fit a Random Forest model with adjusted hyperparameters
rf_model_adjusted <- ranger(
  formula = train_target ~ ., 
  data = train_data, 
  importance = "impurity", 
  num.trees = 500,  
  min.node.size = 3,  
  mtry = floor(sqrt(ncol(train_data)))  # Use the square root of the number of features
) # controls the number of features (predictors) that are randomly sampled

# Print the model summary
print(rf_model_adjusted)

# Make predictions on the test data
predictions <- predict(rf_model_adjusted, data = imputed_test)$predictions

# Prepare data for Kaggle submission
kaggle_submission <- data.frame(SK_ID_CURR = imputed_test$SK_ID_CURR, TARGET = predictions)

# Write out the file for Kaggle submission
vroom::vroom_write(x = kaggle_submission, file = "./RF_mtry_adjusted.csv", delim = ",")
```
> Kaggle Score: .67384
